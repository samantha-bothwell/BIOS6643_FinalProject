---
title: "BIOS 6643 Project Data Analysis Plan"
author: |
 | Analysts: Samantha Bothwell
 |
 | Report generated: `r format(Sys.Date(), '%m/%d/%Y')`
output:
  html_document:
   #  highlight: espresso
    number_sections: yes
    theme: yeti
    toc: yes
    toc_depth: 2
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

# Introduction

Individuals who classified as overweight or obese were enrolled into the study to understand factors that contribute to weight loss. Participants were asked to step on a bluetooth scale once a day over the course of the study. Within the study, there are 3 cohorts. These cohorts indicate participants who started the study around the same time. 

## *Cohort 1 (N = 29)*
<ul>
  <li> 29 people beginning 04/10/2018 – 07/01/2018 (All but 1 began on in April and May) </li>
  <li> Ending between 11/14/2018 – 02/20/2020 </li>
</ul>

## *Cohort 2 (N = 26)*
<ul>
  <li> 26 people beginning 02/13/2019 – 09/30/2019 (All but 2 began between 02/13/2020 – 02/16/2020) </li>
  <li> Ending between 12/22/2019 – 04/20/2020 (All but 2 ended between 04/05/2020 – 04/20/2020) </li>
</ul>

## *Cohort 3 (N = 36)*
<ul>
  <li> 36 people beginning 09/30/2019 – 11/05/2020 </li>
  <li> Ending between 12/02/2019 – 04/20/2020 (All but 2 ended in March or April) </li>
</ul>

The research questions of interest are : 
<ul>
  <li> What is the trajectory of weight over the duration of time in the study? </li>
  <li> Is there a relationship between month of study and weight loss, when accounting for sex and age? </li>
</ul>

# Methods

## Data Cleaning 

## Data Analysis

For this project I aim to use a Functional Data Analysis approach, a method often utilized for time series data. This would incorporate fitting a curve to the longitudinal data and analyzing its behavior. A benefit to this approach is that it is more flexible to handle irregular, on unequally spaced, data than traditional statistical analyses. 

Analysis will be performed in R, version 4.0.2 as well as SAS 9.4.

There are many ways the data are correlated. 
<ul>
  <li> Recordings by individual </li>
  <li> Cohort 1 weights were all taken before COVID. Cohorts 2 and 3 had their weights recorded before and during COVID. </li>
  <li> There may be a correlation based on the month. For example, individuals may be more incentized to lose weight during January because of New Years. Or, warmer months may encourage people to workout than during colder months. </li>
<ul>

When analyzing data, it is ideal to have regular data. Regular data are data where measurements are taken on a consistent timeline, without gaps. In this dataset, individuals have differing days of data and skip recording on some days. To account for this, I will need to research how to properly handle irregular data for analysis in the functional framework. FDA incorporates B-spline bases to construct smoothed functions. I will also use this project to analyze the difference between modeling the data with splines and a functional model. 

Cohorts were grouped by start date as well as length of time in the study. Both cohorts 2 and 3 ended at the same time, due to COVID-19, but started at varying times. The given cohorts are not clearly defined by start date. For example, in cohort 2 25/27 participants started the study between 2/13/2020 - 2/16/2020 and 25/27 participants ended the study between 4/5/2020 - 4/20/2020. For the individuals who do not fall within the same interval as the majority, it is not clear yet how they should be grouped.   
It may be more meaningful to simply look at month in the study, rather than the cohort groupings. This is something I plan to investigate further. 


# Results

Based on the spaghetti plot, printed below, the biggest difference between the 3 cohorts is the length of study. Cohort 3 has an average length in study of 178.35 days. Cohort 2 has an average length in study of 412.85 days. Cohort 1 has the longest average length in study of 546.6 days. A reason why cohort may be important is because all the cohorts seem to show a decline in weights from 1 - 200 days. Cohorts 1 and 2 seem to flatten out after 200 days. Cohort 3 does not seem to flatten out due to its short study length. An additonal reason to consider cohort, as mentioned before, is because COVID-19 could confound the weight loss trajectory since stay at home orders could inhibit someone's ability to lose weight. 

```{r, fig.height = 7}
#### Load in the data 
wt <- read.csv("D:/CU/Fall 2020/BIOS 6643/Project/BIOS6643_FinalProject/DataProcessed/daily_weights_with_confounding.csv")
wt2 <- read.csv("D:/CU/Fall 2020/BIOS 6643/Project/BIOS6643_FinalProject/DataProcessed/daily_weights_clean_wk.csv")


#### Plot data x-axis: study_days, y-axis: wt_lb, group by: participant_id
library(ggplot2)
library(gridExtra)

g1 <- ggplot(data = wt, aes(x = study_days, y = wt_lb, group = participant_id, color = as.factor(cohort))) + 
  geom_line()


#### Plot data x-axis: weight_dates, y-axis: wt_lb, group by: participant_id 
g2 <- ggplot(data = wt, aes(x = as.Date(weight_dates), y = wt_lb, group = participant_id, color = as.factor(cohort))) + 
  geom_line() 


grid.arrange(g1, g2, ncol = 1)
```

```{r}
# Get last measurement 
library(dplyr)
dem <- wt %>% 
  group_by(participant_id) %>% 
  arrange(study_days) %>% 
  summarize(`Total Measures` = n(), `Time Span` = max(study_days, na.rm = T), Cohort = cohort,
    `Baseline Weight` = first(na.omit(wt_lb)), Age = age, Sex = sex, Race = race, Ethnicity = ethnicity, 
    `Marital Status` = marital_status) %>% 
  slice(1)

# Relevel factors 
dem$Sex <- as.factor(dem$Sex); dem$Race <- as.factor(dem$Race)
dem$Ethnicity <- as.factor(dem$Ethnicity); dem$`Marital Status` <- as.factor(dem$`Marital Status`)
dem$Sex <- recode_factor(dem$Sex, "0" = "Female", "1" = "Male")
dem$Race <- recode_factor(dem$Race, "1" = "American Indian or Alaska Native", "2" = "Asian", "3" = "Black", 
  "4" = "Native Hawaiian or other Pacific Islander", "5" = "White", "6" = "Not Reported", "7" = "Other")
dem$Ethnicity <- recode_factor(dem$Ethnicity, "1" = "Hispanic or Latino", "2" = "Non Hispanic or Latino", 
  "3" = "Other")
dem$`Marital Status` <- recode_factor(dem$`Marital Status`, "1" = "Single", "2" = "Committed Relationship", 
  "3" = "Married", "4" = "Separated", "5" = "Divorced", "6" = "Widowed", "7" = "Not Reported")

dem <- dem[dem$participant_id %in% wt2$participant_id,]

# table 1 by cohort 
library(table1)

dem$Cohort <- factor(dem$Cohort,
    levels = c(1,2,3,4), 
    labels = c("Cohort 1", "Cohort 2", "Cohort 3", "P-Value"))

# Format table
rndr <- function(x, name, ...) {
    if (length(x) == 0) {
        y <- dem[[name]]
        s <- rep("", length(render.default(x=y, name=name, ...)))
        if (is.numeric(y)) {
            p <- kruskal.test(y ~ dem$Cohort)$p.value
        } else {
            p <- chisq.test(table(y, droplevels(dem$Cohort)))$p.value
        }
        s[2] <- sub("<", "&lt;", format.pval(p, digits=3, eps=0.001))
        s
    } else {
        render.default(x=x, name=name, ...)
    }
}

# Format table
my.render.cont <- function(x) {
    with(stats.apply.rounding(stats.default(x), digits=2), c("",
        "Mean (95% CI)"=sprintf("%s (%s, %s)", 
        MEAN, round(quantile(x, 0.025, na.rm = T),2), round(quantile(x, 0.975, na.rm = T),2))))
}

dem <- dem[!(dem$participant_id == "KBU-082"),]

# make table 
tb1 <- table1(~ `Age` + `Sex` + `Race` + `Ethnicity` + 
              `Baseline Weight` + `Time Span` |`Cohort`,
              data = dem, render=rndr, droplevels=F, overall = F)
tb1
```

# Discussion 

# References 

<table>
 <thead>
  <tr>
   <th style="text-align:left;"> Missing data analysis: mean.wt </th>
   <th style="text-align:left;">   </th>
   <th style="text-align:right;"> Not missing </th>
   <th style="text-align:right;"> Missing </th>
   <th style="text-align:right;"> p </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> cohort </td>
   <td style="text-align:left;"> 1 </td>
   <td style="text-align:right;"> 1311 (86.5) </td>
   <td style="text-align:right;"> 204 (13.5) </td>
   <td style="text-align:right;"> &lt;0.001 </td>
  </tr>
  <tr>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;"> 2 </td>
   <td style="text-align:right;"> 1197 (93.6) </td>
   <td style="text-align:right;"> 82 (6.4) </td>
   <td style="text-align:right;">  </td>
  </tr>
  <tr>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;"> 3 </td>
   <td style="text-align:right;"> 869 (90.1) </td>
   <td style="text-align:right;"> 96 (9.9) </td>
   <td style="text-align:right;">  </td>
  </tr>
  <tr>
   <td style="text-align:left;"> sex </td>
   <td style="text-align:left;"> 0 </td>
   <td style="text-align:right;"> 2540 (89.1) </td>
   <td style="text-align:right;"> 310 (10.9) </td>
   <td style="text-align:right;"> 0.012 </td>
  </tr>
  <tr>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;"> 1 </td>
   <td style="text-align:right;"> 837 (92.1) </td>
   <td style="text-align:right;"> 72 (7.9) </td>
   <td style="text-align:right;">  </td>
  </tr>
  <tr>
   <td style="text-align:left;"> race </td>
   <td style="text-align:left;"> 2 </td>
   <td style="text-align:right;"> 135 (96.4) </td>
   <td style="text-align:right;"> 5 (3.6) </td>
   <td style="text-align:right;"> 0.004 </td>
  </tr>
  <tr>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;"> 3 </td>
   <td style="text-align:right;"> 233 (87.9) </td>
   <td style="text-align:right;"> 32 (12.1) </td>
   <td style="text-align:right;">  </td>
  </tr>
  <tr>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;"> 5 </td>
   <td style="text-align:right;"> 2878 (89.5) </td>
   <td style="text-align:right;"> 339 (10.5) </td>
   <td style="text-align:right;">  </td>
  </tr>
  <tr>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;"> 7 </td>
   <td style="text-align:right;"> 131 (95.6) </td>
   <td style="text-align:right;"> 6 (4.4) </td>
   <td style="text-align:right;">  </td>
  </tr>
  <tr>
   <td style="text-align:left;"> age </td>
   <td style="text-align:left;"> Mean (SD) </td>
   <td style="text-align:right;"> 41.9 (9.3) </td>
   <td style="text-align:right;"> 44.6 (9.8) </td>
   <td style="text-align:right;"> &lt;0.001 </td>
  </tr>
</tbody>
</table>
